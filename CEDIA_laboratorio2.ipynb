{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CEDIA_laboratorio2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNX4TVfaJlNCvGGdFAJ/gRN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asuskf/CEDIA/blob/main/CEDIA_laboratorio2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLG8YL8DV8J"
      },
      "source": [
        "## TF-IDF\n",
        "(InstintoProgramador, 2019)\\\n",
        "**TF** significa: \"frecuencia de término\" \\\n",
        "**IDF** significa: \"frecuencia de documento inversa\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwWoCxZfFYC_"
      },
      "source": [
        "### Pasos para TF-IDF\n",
        "\n",
        "1.   **Tokenización:** \n",
        "Consiste en dividir el texto en las unidades que lo conforman, en este caso, las palabras. \n",
        "2.   Encuentra los valores de TF-IDF\n",
        "        1.   **TF:** La frecuencia con la que se repite una palabra en todo el texto \\\n",
        "Se calcula: (Frecuencia de una palabra en la oración) / (Por el número total de palabras en la oración )  \n",
        "        2.   **IDF:** La frecuencia de documentos inversos \\\n",
        "        Se calcula:  log((Número total de frases (documentos))) / (Número de frases (documentos) que contienen la palabra)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNVRJJmqAVEO",
        "outputId": "42a43b2b-deea-4db2-b25f-4ab07bb053d8"
      },
      "source": [
        "# Ejemplo de (Programadorclic,s.f.)\n",
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "#Cuerpo  \n",
        "corpus = [  \n",
        "    'Este es el primer documento.',\n",
        "     'Este es el segundo segundo documento.',\n",
        "     'Y el tercero.',\n",
        "     '¿Es este el primer documento?',  \n",
        "]  \n",
        "#Convierta palabras en texto en matriz de frecuencia de palabras  \n",
        "vectorizer = CountVectorizer()  \n",
        "#Cuenta el número de apariciones de una palabra  \n",
        "X = vectorizer.fit_transform(corpus)  \n",
        "# Obtenga todas las palabras clave de texto en la bolsa de palabras  \n",
        "word = vectorizer.get_feature_names()  \n",
        "print(word)  \n",
        "#Ver resultados de frecuencia de palabras  \n",
        "print(X.toarray()  )\n",
        " \n",
        "# ----------------------------------------------------\n",
        "print('-----------------------') \n",
        "from sklearn.feature_extraction.text import TfidfTransformer  \n",
        " \n",
        "# Llamada de clase  \n",
        "transformer = TfidfTransformer()  \n",
        "print(transformer)\n",
        "#Cuenta la matriz de frecuencia de palabras X en el valor TF-IDF  \n",
        "tfidf = transformer.fit_transform(X)  \n",
        "#Ver estructura de datos tfidf [i] [j] representa el peso tf-idf en el texto de tipo i  \n",
        "print(tfidf.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['documento', 'el', 'es', 'este', 'primer', 'segundo', 'tercero']\n",
            "[[1 1 1 1 1 0 0]\n",
            " [1 1 1 1 0 2 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [1 1 1 1 1 0 0]]\n",
            "-----------------------\n",
            "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
            "[[0.43877674 0.35872874 0.43877674 0.43877674 0.54197657 0.\n",
            "  0.        ]\n",
            " [0.27230147 0.22262429 0.27230147 0.27230147 0.         0.85322574\n",
            "  0.        ]\n",
            " [0.         0.46263733 0.         0.         0.         0.\n",
            "  0.88654763]\n",
            " [0.43877674 0.35872874 0.43877674 0.43877674 0.54197657 0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRdgiSOzGe1B"
      },
      "source": [
        "El Tf-IDF: **No captura ninguna relación entre las palabras, cada palabra está aislada entre sí basado en la frecuencia que esta aparezca.** \\\n",
        "Cada palabra se aprende de manera única en función de la frecuencia en la que aparece en las oraciones que se está analizando, si por algún motivo esta palabra no existe dentro del corpus (datos con el que se creó) dará un error de vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBPWlAZZ2cRB"
      },
      "source": [
        "## Método FastText\n",
        "(Amit C, 2020) La idea clave se basa en utilizar la estructura interna de una palabra para mejorar las representaciones vectoriales obtenidas con el método de omisión de gramática.\n",
        "El presente artículo presenta los siguientes métodos:\n",
        "1.   **N-gram:** Se toma una palabra, se define una ventana y se crea los n-grams \\\n",
        "\n",
        "> **Ejm:** caballo con ventana de 3:\\\n",
        " cab - aba - bal - all -llo \n",
        "\n",
        "2.   **Skip-gram:** Toma la palabra central de una frase y va aprendiendo sus palabras cercanas (vecinoas). \n",
        "\n",
        "> **El** caballo **blanco**.\n",
        "Toma a la palabra caballo y va aprendiendo sus palabras cercanas en este caso El y blanco\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5mcoyfm2R_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9887c603-0c9e-49be-a71f-b7ee72a136e5"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbrxnLzv2vEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db24b4f-b3c4-48e7-aa0a-ce390c556fcb"
      },
      "source": [
        "## Descargamos un modelo entrenado de wikipedia\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-27 13:35:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5445405108 (5.1G) [application/zip]\n",
            "Saving to: ‘wiki.es.zip’\n",
            "\n",
            "wiki.es.zip         100%[===================>]   5.07G  42.5MB/s    in 2m 2s   \n",
            "\n",
            "2021-09-27 13:37:38 (42.6 MB/s) - ‘wiki.es.zip’ saved [5445405108/5445405108]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JipON73fJA5"
      },
      "source": [
        "Más dumps en español datos de wikipedia: https://dumps.wikimedia.org/eswiki/20210920/\n",
        "\n",
        "Modelos entrenados por fastext https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYG-ijsU2xpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582650f5-8c13-4b29-fb9f-e0bf33336159"
      },
      "source": [
        "!unzip wiki.es.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wiki.es.zip\n",
            "replace wiki.es.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace wiki.es.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvsx-6JZ20cr"
      },
      "source": [
        "!cp wiki.es.bin /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wufx2yuv24sJ"
      },
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "from gensim.models.fasttext import load_facebook_model\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models.fasttext import save_facebook_model\n",
        "start = datetime.datetime.now()\n",
        "cap_path = datapath(\"wiki.es.bin\")  \n",
        "fb_model = load_facebook_model(cap_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KuXmUFMYNx7"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgpvq426ZDTp"
      },
      "source": [
        "def get_Text_Link(link):     \n",
        "    response = requests.get(link)\n",
        "    response = response.text.split('\\n')\n",
        "    cleanAnswer = [text.replace('  ',' ').replace('-“', '').replace('.“', '').replace('” ',' (') for text in response if len(text) > 1]\n",
        "    return cleanAnswer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF690glEZGEQ"
      },
      "source": [
        "#Libro tomado de (jsdario,2017) \n",
        "link = 'https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt'\n",
        "book = get_Text_Link(link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9DJPw6s3go5"
      },
      "source": [
        "newCorpus = [sentence for sentence in book]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA831eVsdovH",
        "outputId": "a67f9ea9-f7de-4e17-aef5-b5c03dac0a90"
      },
      "source": [
        "# Añadimos nuevo vocabulario a un modelo ya entrenado\n",
        "fb_model.build_vocab(newCorpus, update=True)\n",
        "fb_model.train(newCorpus, total_examples=len(newCorpus), epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6024937, 103621100)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djDcBXvHhAyU"
      },
      "source": [
        "# Guardamos el modelo nuevo\n",
        "save_facebook_model(fb_model, 'fbEnolaWiki2.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzw_VYfQhmjI"
      },
      "source": [
        "\n",
        "## Transformes\n",
        "\n",
        "(The machine learners, s.f.)\n",
        "El Transformer se basa en un Encoder y un Decoder. La idea clave aquí es que se utiliza el encoder para analizar el contexto de la secuencia de entrada y el decoder es el encargado de generar la secuencia del output a partir de este contexto.\n",
        "\n",
        "\n",
        "\n",
        "1.   **La codificación:**\n",
        "> \n",
        "\n",
        "*   Se realiza un embedding de la secuencia de las palabras para convertir cada una en un vector y tener una representación numérica.\n",
        "*   Añadir la componente posicional para cada vector de palabra.\n",
        "* Aplicar el mecanismo de auto-atención de múltiples cabezas.\n",
        "Capa neuronal Feed Forward.\n",
        "\n",
        "2.   **La decodificación:**\n",
        "> \n",
        "* Se realiza un embedding del resultado del punto temporal justo anterior (t-1).\n",
        "* Se añade la componente posicional.\n",
        "* Se aplica el mecanismo de auto-atención de múltiples cabezas a la secuencia del output de t-1.\n",
        "* Se recoge la salida del encoder para el punto temporal t, aplicamos de nuevo el mecanismo de auto-atención, esta vez utilizando las palabras del output t-1 como V, y la salida del encoder (en t) como K y Q.\n",
        "* Capa neuronal de Feed Forward.\n",
        "* Finalizamos con una capa lineal y una Softmax para obtener la probabilidad de la siguiente palabra y devolver aquella con la probabilidad más alta como la siguiente palabra.\n",
        "\n",
        "(Alejandro V, s.f.) **Capas de atención:** Codifican cada palabra de una frase en función del resto de la secuencia, permitiendo así introducir el contexto en la representación matemática del texto, motivo por el cual a los modelos basados en Transformer se les denomina también Embeddings Contextuales.\n",
        "\n",
        "Modelos entrenados y listos para ser usados \n",
        "https://tfhub.dev/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RisysYu7w4OR"
      },
      "source": [
        "# Modelo y ejemplo tomado de https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Cargamos el modelo desde tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Construcción de modelos para codificar textos en vectores de alta dimensión \n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "model.summary()\n",
        "\n",
        "# Codificación de oraciones multi lenguajes. \n",
        "english_sentences = tf.constant([\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"])\n",
        "italian_sentences = tf.constant([\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"])\n",
        "japanese_sentences = tf.constant([\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"])\n",
        "\n",
        "english_embeds = model(english_sentences)\n",
        "italian_embeds = model(italian_sentences)\n",
        "japanese_embeds = model(japanese_sentences)\n",
        "\n",
        "# English-Italian similaridad\n",
        "print(tf.tensordot(english_embeds, italian_embeds, axes=[[1], [1]]))\n",
        "\n",
        "# English-Japanese similaridad\n",
        "print(tf.tensordot(english_embeds, japanese_embeds, axes=[[1], [1]]))\n",
        "\n",
        "# Italian-Japanese similaridad\n",
        "print(tf.tensordot(italian_embeds, japanese_embeds, axes=[[1], [1]]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8v_ZObzFTb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Otros modelos que no los podremos ver por cuestión de tiempo:\n",
        "1.   RNN (Redes neuronales recurrentes)\n",
        "2.   NNLM Feedforward Neural Network Language Model Elemento de lista\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjMJ0FhCEo87"
      },
      "source": [
        "## Referencias\n",
        "InstintoProgramador.(2019).Python para NLP: Crear un modelo TF-IDF desde cero.https://www.instintoprogramador.com.mx/2019/07/python-para-nlp-crear-un-modelo-tf-idf.html\n",
        "\n",
        "Programadorclic.(s.f.).Python usa scikit-learn para calcular TF-IDF.https://programmerclick.com/article/4807171602/\n",
        "\n",
        "Amit,C.(2020). A Visual Guide to FastText Word Embeddings. https://amitness.com/2020/06/fasttext-embeddings/\n",
        "\n",
        "jsdario, (2017).el_quijote.txt. https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79\n",
        "\n",
        "\n",
        "Alejandro, V.(s.f.).Transformers en Procesamiento del Lenguaje Natural. https://www.iic.uam.es/innovacion/transformers-en-procesamiento-del-lenguaje-natural/\n",
        "\n",
        "The machine learners.(s.f.)Transformer: la tecnología que domina el mundo.https://themachinelearners.com/transformer/\n"
      ]
    }
  ]
}